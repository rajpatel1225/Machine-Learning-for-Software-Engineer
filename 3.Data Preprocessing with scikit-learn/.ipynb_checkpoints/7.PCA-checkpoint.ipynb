{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "Learn about PCA and why it's useful for data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Dimensionality reduction\n",
    "Most datasets contain a large number of features, some of which are redundant or not informative. For example, in a dataset of basketball statistics, the total points and points per game for a player will (most of the time) tell the same story about the player's scoring prowess.\n",
    "\n",
    "When a dataset contains these types of correlated numeric features, we can perform principal component analysis (PCA) for dimensionality reduction (i.e. reducing the number of columns in the data array).\n",
    "\n",
    "PCA extracts the principal components of the dataset, which are an uncorrelated set of latent variables that encompass most of the information from the original dataset. Using a smaller set of principal components can make it a lot easier to use the dataset in statistical or machine learning models (especially when the original dataset contains many correlated features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. PCA in scikit-learn\n",
    "Like every other data transformation, we can apply PCA to a dataset in scikit-learn with a transformer, in this case the PCA module. When initializing the PCA module, we can use the n_components keyword to specify the number of principal components. The default setting is to extract m - 1 principal components, where m is the number of features in the dataset.\n",
    "\n",
    "The code below shows examples of applying PCA with various numbers of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefined data\n",
    "print('{}\\n'.format(repr(data)))\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca_obj = PCA() # The value of n_component will be 4. As m is 5 and default is always m-1\n",
    "pc = pca_obj.fit_transform(data).round(3)\n",
    "print('{}\\n'.format(repr(pc)))\n",
    "\n",
    "pca_obj = PCA(n_components=3)\n",
    "pc = pca_obj.fit_transform(data).round(3)\n",
    "print('{}\\n'.format(repr(pc)))\n",
    "\n",
    "pca_obj = PCA(n_components=2)\n",
    "pc = pca_obj.fit_transform(data).round(3)\n",
    "print('{}\\n'.format(repr(pc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code output above, notice that when PCA is applied with 4 principal components, the final column (last principal component) is all 0's. This means that there are actually only a maximum of three uncorrelated principal components that can be extracted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
